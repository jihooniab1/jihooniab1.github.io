---
title: "COSE-322"
date: 2025-11-18 01:28:16 +0900
categories: [KoreaUniv, System_Programming]
tags: []
---

# COSE-322
https://os.korea.ac.kr/

# Index
- [1. Lecture 1](#lecture-1)
- [2. Lecture 2](#lecture-2)
- [3. Lecture 3](#lecture-3)

# Lecture 1
## File
운영체제에서 **파일** 은 저장장치를 사용자에게 쉽게 보여주기 위한 **추상화** 된 개념입니다. 운영체제에서 `File as abstraction`은 **바이트 들의 선형 배열(Linear array of bytes)** 로 정의 됩니다. 
- 선형적(Linear)은 파일 데이터가 1차원 구조로 i, i+1, i+2와 같이 순차적으로 배치되는 것을 의미합니다. 
- 배열(array)은 데이터 공간이 비어있지 않고 연속적(not sparse)이라는 것으로 프로세스의 주소 공간은 sparse 할 수 있다는 점과 차이가 있습니다.
- 파일 내에서 데이터에 접근하는 최소 단위는 **byte** 단위입니다. 물리 메모리가 `word-aligned` 되어 있더라도 파일 시스템은 바이트 단위의 접근을 보장합니다. 

파일은 주소 공간(address space)과 비교했을 때, 파일의 경우 길이가 가변적이고, 비휘발성(persistent)이며 데이터가 연속적이라는 특징이 있습니다. 반면 프로세스의 주소 공간은 길이가 고정(Fixed)되어 있으며 휘발성(Volatile)이고 비연속적일 수 있다는 차이가 있습니다. 파일과 주소 공간 모두 **프로세스 간 공유** 가 가능하다는 공통점이 있습니다.

데이터베이스에서는 failure semantics라는게 확실하게 정의되어 있음. 뭐라도 중간에 하다가 실패하면 원위치로 돌아가기(트랜잭션). 파일 시스템은 **failure semantics** 가 정의되어 있지 않음. ext4 같은 파일 시스템에서 `저널링`이라는 걸로 부분적인 보완을 하지만 DB 수준까지는 아님. 

File로 DB를 만들면 어떻게 될까? => 파일로 DB를 만들 수는 있으나 트랜잭션을 정확히 수행못할 수 있다는 단점이 있습니다.

파일 시스템은 크게 두가지 의미를 갖는데, 하나는 파일과 물리디스크 블록 간의 **mapping** 을 의미합니다. 파일 시스템은 디스크 위치 배치를 관리하고, 사용자는 스토리지의 종류나 파일이 저장된 위치를 알 필요 없습니다(**location independence**). 파일 블록 넘버가 1, 2, 3이런 파일들이 디스크 블록 번호는 108, 3010, 3011과 같은 경우를 의미합니다.

다른 하나는 디스크에 들어가 있는 파일 전체를 총칭하는 것으로 파일 시스템에 따라 파일들의 배치나 구성이 달라집니다. 프로세스와 파일 시스템간의 관계는 아래와 같이 나타낼 수 있습니다. 프로세스는 파일을 사용하는 주체로 읽기, 쓰기 작업을 수행하고, 메모리는 프로세스가 직접 접근하여 작업하는 공간입니다. 프로세스가 파일 데이터를 메모리에 적재합니다. 파일 시스템은 **파일 객체** 와 **버퍼 캐시** 로 구성되며 파일 객체는 프로세스가 접근하는 추성화된 인터페이스, 버퍼 캐시는 `디스크 I/O`를 줄이기 위한 성능 향상 메커니즘입니다.그리고 Physical Storage는 실제 데이터가 저장되는 하드웨어입니다.  <br>

![image](/assets/img/posts/koreauniv-system-programming/Lec1_1.png) <br>

#### 프로세스가 Read 연산 실행
프로세스가 `read` 시스템 콜을 호출하면 파일 시스템은 먼저 **Buffer Cache** 를 확인합니다. 만약 `cache hit`라면 캐시에서 바로 메모리로 복사를 하고, `cache miss`라면 Physical Storage에서 데이터를 읽어온 다음 캐시에 저장하고, 이를 메모리로 복사합니다. 이를 통해 데이터가 프로세스의 메모리 공간으로 복사됩니다. 

#### 프로세스가 Write 연산 실행
프로세스가 `write` 시스템 콜을 호출하면 데이터가 먼저 **Buffer Cache** 에 기록된 후 나중에 Physical Storage에 기록이 됩니다. 디스크 속도는 굉장히 느리기 때문에 **page cache** 와 비슷한 느낌으로 자주 사용되는 데이터를 메모리에 유지한다고 보면 될 것 같습니다. 파일에는 **메타데이터(Metadata)** 라 불리는 파일의 속성을 저장하는 필드가 있습니다. 파일의 메타데이터에는 다음과 같은 값들이 있습니다. <br>

![lec1_2](/assets/img/posts/koreauniv-system-programming/Lec1_2.png) <br>

기타 메타데이터로는 Time, date, user id (uid), Directory 정보 등이 있습니다. 그리고 리눅스 커널에서는 **inode** 라는 자료구조에서 이러한 메타데이터를 저장합니다. 그리고 inode는 디스크에 저장되어 `Persistence`를 제공합니다. 이런 메타데이터도 **caching** 을 하는데, 파일에 접근할 때마다 디스크에서 메타데이터를 읽고, 그 위치 정보를 바탕으로 다시 디스크에 접근하면 성능이 저하됩니다. 이를 해결하기 위해 `TLB`와 유사하게 메타데이터를 메모리에 **caching** 합니다. 이러한 캐싱은 **데이터 불일치(inconsistency)** 문제를 야기할 수 있는데, 이를 극복하기 위해 `jornal`에 임시 저장하는 **저널링 파일 시스템** 이 등장하였습니다. 또한 파일이 삭제될 때는 실제 데이터가 아니라 메타데이터가 먼저 삭제되기 때문에 포렌식 및 복구가 가능합니다. <br>

![lec1_3](/assets/img/posts/koreauniv-system-programming/Lec1_3.png) <br>

파일 시스템이 올라간다.. 라는 것의 의미는 파티션에 메타데이터가 설치되고 뒤에 데이터 블락(파일 블락)들이 따라오는 것이라고 볼 수 있습니다. 이 파티션들은 물리적으로 하나의 저장매체일 수 있고, 나아가서 여러 스토리지들을 묶어서 하나의 파티션으로 정의할 수 있습니다. 메타데이터가 망가지면 전체를 읽지 못할 수 있기 때문에, 복사를 중간 중간 해두는 경우도 있습니다.

### File Operations(Generic)
어떤 파일 시스템을 쓰더라도 다음 동작들은 반드시 있어야 합니다. <br>

![lec1_4](/assets/img/posts/koreauniv-system-programming/Lec1_4.png) <br>

리눅스가 독특한 점은 위치가 **implicit** 하다는 점입니다. write 같은 시스템 콜을 할 때 **file descriptor** 를 사용하는데, 위치를 인자로 갖지는 않습니다. 즉 파일의 현재 위치를 프로그래밍 하는 사람이 알고 있어야 한다는 것입니다. 

- current-file-position 포인터: 파일을 읽거나 쓸 때마다 자동으로 업데이트 되는 포인터로, 파일에 대해 프로세스가 어디까지 작업을 했는지 기록하는 포인터입니다. 이는 **per-process** open-file table에 저장됩니다. 
- lseek: read, read, write 같은 호출을 반복하면 파일 포인터가 계속 뒤로 가는데, 이럴 때 **file seek** 을 이용해서 원하는 위치로 가야합니다. 

### File 사용 순서
open과 close는 여러 프로세스가 안전하게 파일을 공유할 수 있도록 지원하는 중요한 역할을 하며, 이를 위해 커널은 두 가지 테이블을 관리합니다. => **open-file table** <br>

![lec1_5](/assets/img/posts/koreauniv-system-programming/Lec1_5.png) <br>

1. Per-process table
프로세스마다 독립적으로 유지되며, 파일 포인터 등의 **state** 를 저장합니다.

2. System wide
모든 프로세스가 공유하며, 파일 위치, 접근 날짜, 열린 횟수(open count) 등 프로세스에 독립적인 정보를 가집니다. 이 테이블은 디스크의 메타데이터(파일 제어 블록, file control block)를 캐싱하는 역할을 합니다. `open` 시스템 콜은 open count를 1 증가시키고, `close` 시스템 콜은 1 감소시킵니다. 즉, open을 하고 close를 해주지 않으면 이 open count가 0이 되지 않아 삭제 작업을 수행할 수 없게 됩니다. 

## Directory Structure
디렉터리는 파일들을 그룹화하여 관리하는 기법입니다.
- 단일 레벨(single-level) 디렉터리: 초창기 구조로, 모든 파일이 하나의 디렉터리에 존재하였고 하부 디렉터리를 만들 수 없었습니다.
- 트리 구조(tree-structure) 디렉터리: 현대적인 파일 시스템 구조로, 디렉터리 안에 또 다른 sub-directory를 임의로 만들 수 있습니다. 이를 위해서는 **root, path, current directory** 와 같은 개념이 필요합니다. 또한 **asyclic graph** 를 유지해야 합니다.

### Link
링크는 하나의 파일이나 디렉터리를 여러 경로에서 접근할 수 있도록 하는 기능입니다. <br>

![lec1_6](/assets/img/posts/koreauniv-system-programming/Lec1_6.png) <br>

- Soft Link, Symbolic Link: 원본 파일을 가리키는 **경로 정보(symbolic name)** 를 담고 있는 특별한 파일입니다. 바로가기 같은 느낌이고, 링크 파일을 삭제해도 원본 파일은 영향을 받지 않습니다. 소프트링크가 가리키고 있던 원본 파일이 삭제되면 그 링크는 **broken link** 가 됩니다.
- Hard Link: 하나의 파일 데이터(inode)에 여러 개의 이름을 부여하는 느낌으로, 링크를 삭제하면 참조 카운트를 감소시키고 이 카운트가 0이 될 때 실제 파일 데이터가 삭제됩니다. inode에는 이 파일에 연결된 이름(하드 링크)이 몇 개인지 세는 `link count`가 있습니다. 사용자가 파일을 삭제했을 때 만약 카운트가 0이 아니라면, 즉 다른 하드 링크가 남아있다면 inode와 데이터는 디스크에 그대로 남겨둡니다. 카운트가 0이 될 때 비로소 해당 inode와 연결된 데이터 블록을 디스크에서 완전히 삭제하는 것입니다.

![lec1_7](/assets/img/posts/koreauniv-system-programming/Lec1_7.png) <br>

## Mount
마운트는 특정 디렉터리에 다른 장치(device)의 파일 시스템을 연결하는 기능입니다. 파일 시스템을 사용하려면 마운트를 해야 하는데, 비어 있는 `/usr` 디렉터리에 disk2 장치를 마운트하면, disk2의 파일들이 /usr 하위 경로에서 접근 가능해지는 방식입니다.

다음과 같은 장점들이 있습니다.
- 확장성(Scalability): 수백 개의 디스크를 하나의 논리적인 파일 시스템처럼 사용할 수 있습니다.
- 투명성(Transparency): 장치가 변경되거나 추가되어도 사용자는 동일한 경로로 접근하여 사용할 수 있습니다.
- 분산 파일 시스템(Distributed FS)으로 확장 용이: 네트워크 상의 다른 컴퓨터 파일 시스템(ex: NFS)을 로컬 디렉터리에 마운트하여 내 파일처럼 쓸 수 있습니다. 

**Namespace:** 이름을 붙여 대상을 식별하는 공간을 의미합니다. 파일 시스템에서는 파일이나 디렉터리를 서로 겹치지 않게 구분하기 위해 이 개념을 사용합니다. **/a** 와 **/b** 는 서로 다른 디렉터리, 즉 **서로 다른 네임스페이스** 이므로 같은 `cc`라는 이름을 갖는 파일이 `/a/cc`, `/b/cc` 형태로 존재할 수 있습니다. 

일반적으로 시스템에 연결된 모든 저장장치가 마운트 되면 시스템에서 실행되는 모든 프로세스는 **동일한 파일 시스템 구조(네임 스페이스)** 를 보게 됩니다. 그러나, 프로세스의 파일 **file name space** 를 다르게 할 수도 있습니다. **Container** 기술을 사용하면 운영체제는 특정 프로세스에게 실제 시스템의 파일 구조와는 다른, **격리된 가상의 파일 네임 스페이스** 를 할당할 수 있습니다. 컨테이너 안에서 실행되는 프로세스는 컨테이너 내부의 파일 시스템을 root directory로 인식하게 됩니다. 

## Protection, Access Control
파일에 대한 부적절한 접근을 막는 기능입니다. RWX로 나뉩니다. 접근 권한을 부여하기 위해 사용자를 `세 그룹`으로 나눕니다.

- Owner: 파일을 소유한 사용자
- Group: 파일이 속한 그룹의 사용자들(/etc/group에 정의됩니다)
- Public/Others: 그 외 모든 사용자 

Access Control은 **chmod** 로, Owner와 Group은 **chown** 으로 변경할 수 있습니다.

761 => 소유자는 rwx, 그룹은 rw, 다른 사용자는 x(1)

# Lecture 2
## File 디자인
강의에서 다루고 있는 파일 시스템은 **커널에서 사용하는 소프트웨어** 를 의미하는 것입니다. 파일 디자인의 핵심은 **data block** 이 어디 있느냐에 대한 것으로 file의 내용을 담고 있는 **data block** 이 저장되는 디스크상 위치 정보를 결정하는 것과 이에 대한 과리 방법을 주로 다루게 될 것입니다.

Data block이 저장되는 위치의 할당 기법에는 다음 항목들이 있습니다.
- Contiguous Allocation(연속 할당)
- Linked List Allocation(연결 리스트 할당)
- Linked List Allocation Using Index(인덱스를 사용한 연결 리스트 할당)
- Inode

### Contiguous Allocation
파일을 물리적으로 연속된 disk block에 저장하는 방식입니다. 프로그래밍에서 기억 공간을 할당할 때 **고정 길이 배열** 을 사용하는 것와 유사합니다. 디렉터리는 각 파일의 **시작 위치** 와 **블록 길이** 만 기억하고 있으면 됩니다. 구현이 간단하고, 전체 파일을 한번에 읽어 들일 경우에 성능이 매우 뛰어나다는 장점이 있으나, 파일은 반드시 한번에 끝까지 기록되어야 하고, 확장을 위하여 파일의 끝에 예비용 block을 남겨두는 경우 공간 낭비가 발생한다는 단점이 있습니다. <br>

![lec1_8](/assets/img/posts/koreauniv-system-programming/Lec1_8.png) <br>

위 그림의 경우, 블록이 삭제될 때 반드시 파일들은 연속적으로 할당되어야 하여 `Massive Copy`가 발생하는 것을 볼 수 있습니다. <br>

![lec_9](/assets/img/posts/koreauniv-system-programming/Lec1_9.png) <br>

디스크에서 **빈 공간(disk free block)** 을 찾을 때는 **Bitmap** 을 이용합니다. 블락이 사용 중이면 0으로 표시하는 방식입니다. 컴퓨터는 비트맵을 워드 단위로 처리를 하는데, **#zero word** 는 모든 비트가 사용 중인 워드를 의미합니다. 그렇기에 (Number of bits per word) * `#zero word` + `offset of first 1`으로 free block을 찾습니다. 

### Linked List Allocation
Disk의 block을 linked list로 구현하여 file의 data를 저장하도록 하는 방식입니다. File의 data block이 disk의 어디든지 위치할 수 있고, 연속 할당과 달리 공간의 낭비가 없다는 장점이 있습니다. 그러나 Random access가 불가능합니다. 즉, File의 특정 위치를 찾기 위해서는 해당 file의 시작 노드부터 찾아가야 합니다. 이를 해결하려면 File의 메타데이터와 내용을 **In-memory object** 로 구성할 수 있습니다. 그리고 다음 data block에 대한 포인터를 저장하는 공간 역시 추가로 필요합니다. 아래 그림에서 disk block pointer의 크기를 결정하는 요소는 **전체 디스크 블록의 개수, 디스크 크기와 블록 크기** 라고 할 수 있습니다. <br>

![lec_10](/assets/img/posts/koreauniv-system-programming/Lec1_10.png) <br>

Linked List Allocation 방식의 예로는 MS-DOS의 **FAT(File Allocation Table)** 파일 시스템을 들 수 있습니다. 아래 그림을 설명하자면, test라는 이름이 파일의 시작 블록은 217이고 FAT에 따라 **217 -> 618 -> 339** 블록 순서로 파일이 구성되는 것입니다. <br>

![lec1_11](/assets/img/posts/koreauniv-system-programming/Lec1_11.png) <br>

### Linked List Allocation Using Index
이 방식은 file의 data block의 위치를 별도의 block에 모아두는 것입니다. file의 data block 중 하나를 **index block** 이라 하여, 모든 data block의 위치를 index block에서 알 수 있게 하는 것입니다. 이전에 봤던 Linked List Allocation과 비교하여, random access시 하나의 data block에서 찾아가고자 하는 data block의 위치를 알 수 있으므로 **더 빠르게 random access** 가 가능합니다. 그러나 Index Block의 크기가 고정되어 있어 수용할 수 있는 포인터 수에 한계가 있고, 그렇기에 최대 파일의 크기가 고정이 됩니다. 

### inode
inode는 file에 대한 data block index를 계층 형태로 관리하는 방식입니다. 큰 메모리 영역을 관리할 때 1-level paging 보다 **multi-level paging** 이 유리한 것과 비슷한 개념입니다. inode의 구성 요소는 다음과 같습니다.
- File에 대한 속성 나타내는 field: mode, owners, timestamps, size, count...
- 작은 크기의 파일을 위한 direct index
- 파일의 크기가 커짐에 따라서 요구되는 data block의 index들을 저장하기 위한 index table들
    - single indirect block
    - double indirect block
    - triple indirect block
직접 블록이 아닌 간접 간접 참조 블록의 경우 inode -> 포인터 블록 -> 포인터 블록 -> data 블록들 이런 방식으로 관리를 합니다. <br>

![lec1_12](/assets/img/posts/koreauniv-system-programming/Lec1_12.png) <br>

inode의 용량을 계산해보겠습니다. 일단 블록 사이즈가 4KB, 직접 블록의 개수는 12, 포인터 크기가 4 바이트라고 가정을 하겠습니다.

- Direct Blocks: 4KB * 12 => 48KB
- Single Indirect: 4KB * 1024 => 4MB
- Double Indirect: 4KB * 1024 * 1024 => 4GB
- Triple Indirect: 4KB * 1024 * 1024 * 1024 => 4TB

## Directory 구현
**Directory Entry**: directory를 표현하기 위한 자료구조를 의미합니다. 파일 시스템에 따라서 directory entry를 구성하는 field도 달라집니다. 일반적인 directory entry는 파일의 이름, 속성과 같은 정보가 저장되지만 inode를 사용하는 경우에는 file name과 inode number만 저장이 됩니다.

### Directory in MS-DOS
![lec1_13](/assets/img/posts/koreauniv-system-programming/Lec1_13.png) <br>

File의 구현이 linked list allocation인 MS-DOS에서 사용하는 directory entry는 위 그림과 같습니다. **first block number** 를 통해 맨 처음 data block만 알아내면 전체 file의 data를 알 수 있게 됩니다.

### Directory in Linux
![lec1_14](/assets/img/posts/koreauniv-system-programming/Lec1_14.png) <br>
MS-DOS와 다르게 file의 속성, ownership 같은 정보는 해당하는 file의 inode 자료구조에 들어있기 때문에 directory entry에는 `file 이름`과 `inode number`가 있습니다. 이제 리눅스에서 directory lookup을 하는 과정을 알아보겠습니다. 예시로 들 경로는 **/usr/ast/mbox** 파일 입니다. <br>

![lec1_15](/assets/img/posts/koreauniv-system-programming/Lec1_15.png) <br>

먼저 루트 디렉토리에서 `usr` 디렉터리의 inode 번호인 6을 알아냅니다. 그리고 inode 6을 확인하면 direct blocks 필드의 132 값을 통해 /usr 디렉터리를 나타내고 있는 것이 132 블록임을 알아낼 수 있습니다. 이 과정을 반복하여 inode 60이 가리키고 있는 실제 데이터 블록을 확인하면 파일 내용을 읽을 수 있게 됩니다. 그러나 위 과정에서 알 수 있듯 상당히 많은 disk I/O가 발생하기 때문에 **directory cache** 를 통해 디스크 접근 횟수를 대폭 감소시킬 수 있습니다. 자주 접근하는 경로의 inode와 directory entry를 메모리에 캐싱하는 것입니다. 

# Lecture 3
## File System의 계층화
![lec3_1](/assets/img/posts/koreauniv-system-programming/Lec3_1.png) <br>

파일 시스템을 계층화 하면 코드 재사용성 극대화할 수 있습니다. **I/O control** 계측은 모든 파일 시스템에서 공통으로 사용하기에 파일 시스템 코드의 중복을 최소화 할 수 있습니다. 이제 각 계층을 알아보겠습니다.

1. Logical file system
파일 시스템 메타 데이터를 관리합니다(파일 이름, 크기, 권한, timestamp, inode 등)

2. File-organization
**논리 블록 주소를 물리 블록 주소로 변환** 하는 계층입니다. 논리 블록은 파일 내에서 0부터 N까지 순차적을 번호를 매긴 블록을 의미하고, 물리 블록은 실제 저장장치의 물리 주소를 의미합니다. 사용자가 파일의 100번째 블록을 요청하면 디스크의 5432번 블록 주소로 바꿔주는 이런 느낌입니다.

3. Basic file system
장치 드라이버에게 저장장치의 **물리 블록을 읽고 쓰도록 명령** 을 내리는 계층입니다. File-organization에서 5432번을 읽으라는 명령을 주면 장치 드라이버에게 5432번 블록을 읽으라는 명령을 내립니다. 

4. I/O Control
**하드웨어와 직접 통신** 하는 계층입니다. 장치 드라이버가 저장장치 하드웨어에 맞게 명령어를 전달합니다. 예를 들어, Basic file system에서 5432번 블록을 읽으라는 명령을 주면 I/O control 계층에서 하드웨어에 맞는 명령어로 변환을 하고, Device는 실제 디스크 헤드를 이동하여 이를 읽습니다. 

## VFS(Virtual File System)
VFS는 다양항 논리 파일 시스템을 추상화하고, 여러 파일 시스템에 대해 **동일한 system call interface(API)** 를 제공합니다. 이러한 API를 통해 VFS는 여러 파일 시스템을 uniform하게 사용하고, 여러 파일 시스템이 있어도 1개만 있는 것처럼 프로그래밍을 할 수 있습니다. 즉 **object-oriented** 방식을 사용하는 것입니다. 
```
// VFS 구조체
struct file_operations {
    int (*read)(file, buffer, size);   // 함수 포인터
    int (*write)(file, buffer, size);  // 함수 포인터
};

// ext4 구현
struct file_operations ext4_ops = {
    .read = ext4_read,
    .write = ext4_write
};

// NTFS 구현
struct file_operations ntfs_ops = {
    .read = ntfs_read,
    .write = ntfs_write
};

// VFS는 이렇게 호출합니다
file->ops->read(file, buffer, size); 
```

## 파일 시스템의 자료구조
![lec3_2](/assets/img/posts/koreauniv-system-programming/Lec3_2.png) <br>

파일 시스템에는 두 가지 자료구조가 있습니다: **On-disk**, **In memory**

### On-disk
디스크에 영구 저장되는 자료구조입니다. 각 요소를 알아봅시다.
1. Boot block
디스크의 첫 번째 block으로 운영체제가 시작하기 위해 필요한 정보를 가지고 있습니다(부트로더 코드, 커널 위치, 초기화 정보 등..)
2. Super block
파일 시스템 관련 정보들이 어디에 저장되어 있는지에 대한 메타데이터입니다. 파일 시스템 타입, Inode 테이블 위치, 전체 블록 개수, 남은 블록 개수 등이 있습니다. **파일 시스템 당 하나씩** 주어지고, `/` 위치를 가지고 있습니다. 손상되면 복구가 안되기 때문에 중복해서 저장하며 마운트 시에 메모리에 복사되어 메모리 버전이 사용이 됩니다. **inode table** 의 위치를 저장하고 있습니다. 
3. File Control Block (FCB)
Linux File System에서는 FCB는 **inode** 로 구현됩니다. 파일의 모든 메타데이터를 저장합니다. 
4. Directory structure
파일들을 파일 시스템 내에서 organize 하기 위한 자료구조 입니다. **inode + 파일 이름** 으로 directory structure를 구성합니다. 

![lec3_3](/assets/img/posts/koreauniv-system-programming/Lec3_3.png) <br>

위 그림은 파일 시스템의 디스크 레이아웃입니다. **inode table blocks** 는 inode를 모아놓은 block을 의미하며, 여러 block을 사용해 inode들을 저장합니다. 그림을 보면 Super Block 백업도 있는 것을 확인할 수 있습니다. 파일 접근의 디스크 I/O를 계산해보면 
1. Super Block 읽기 (inode 테이블 위치 확인)
2. Inode 읽기 (파일 메타데이터)
3. Data Block 읽기 (실제 파일 내용)

못해도 3번의 disk access가 필요하다는 것을 알 수 있습니다. 성능 향상을 위해 memory에 caching을 할 수 있습니다. => **Buffer Cache**, 자주 사용하는 블록을 메모리에 캐싱하여 디스크 I/O를 줄일 수 있습니다.

### Super Block
Super block은 **파일 시스템 당 하나** 주어지는 블록입니다. `/(루트)` 위치 정보를 포함하고 있으며, 보통 각 디스크 파티션의 첫 블록에 위치하고 있습니다. 그리고 손상되면 복구가 안 되기 때문에 **중복해서 저장(백업)** 됩니다. 

그리고 마운트 될 때 메모리에 복사가 되어, 메모리 버전이 사용이 되는데 **Disk Superblock** 과 **Memory Superblock** 으로 구분합니다. **inode table** 의 위치를 저장하고 있습니다.

### In memory
In-memory structure의 목적은 다음과 같습니다.
1. 파일 시스템 관리
2. Caching을 통한 성능 향상

주요 구조는 다음과 같습니다.
- System-wide open-file table: **시스템 전체에서 open된 모든 파일** 목록, 정보를 가지고 있습니다.
- Per-process open-file table: 프로세스가 open한 파일에 대한 정보를 관리하며, **프로세스마다 독립적** 입니다.
- File descriptor: 사용하고 있는 file에 대한 per-process open-file table에서 **index** 를 의미합니다. 보통 index 0,1,2는 `stdin`, `stdout`, `stderr`로 사용됩니다. 

## Dentry(Directory Entry)
In memory 자료구조: 디렉토리 접근을 위해 On-disk의 super block과 inode에 접근하여 dentry를 생성합니다. `/tmp/foo` 같은 경로가 있을 때, foo 파일에 한번 접근하려면 **여러 번의 dentry 접근**, 즉 여러번의 디스크 I/O를 필요로 합니다. 그렇기에 관련 디렉토리 내용을 메모리에 caching 하여 두 번째 접근 때는 dentry에 접근할 필요 없이 빠르게 작업을 할 수 있도록 합니다. 

## Buffer Cache
Buffer Cache는 **최근에 사용된 disk block을 메모리 공간에 caching** 하는 것을 의미합니다. 디스크 I/O의 시간은 메모리 접근 시간보다 훨씬 크고, 디스크 블락에 접근하는 패턴에도 **locality** 가 있기 때문에, buffer cache를 통해 read, write 같은 호출의 반응 시간을 줄일 수 있습니다. 

특징으로는 다음이 있습니다.
1. VM과 달리 MMU와 같은 하드웨어의 지원을 받지 않습니다.
2. 그리고 `read`, `write`의 연산 수행의 결과와 실제 disk의 내용은 **asynchronous** 합니다. 

### Buffer Cache Policy
캐시의 공간은 보통 physical memory의 1~10 퍼센트 정도만 할당을 합니다. 공간이 한정되기 때문에 **cache entry 교체 알고리즘** 을 필요로 하며, 디스크 접근에도 locality가 나타나기에 대부분 **LRU(Least Recently Used)** 를 사용합니다. 

대부분의 애플리케이션은 buffer cache의 도움을 받을 수 있으나, `DBMS`와 `multimedia application`은 오히려 buffer cache 없이 디스크에 바로 접근하는 것이 이득이 될 수도 있습니다. 버퍼 캐시의 전체 동작 구조는 다음과 같습니다. <br>

![lec3_4](/assets/img/posts/koreauniv-system-programming/Lec3_4.png) <br>

Hash Table은 블록 번호를 빠르게 찾기 위한 해시 테이블을 의미합니다. read, write 시스템 콜이 어떻게 진행되는지도 살펴보겠습니다. <br>
![lec3_5](/assets/img/posts/koreauniv-system-programming/Lec3_5.png) <br>

1. 애플리케이션에서 read를 호출합니다.
2. VFS에서는 read에서 전달받은 인자(fd)를 data의 위치 정보인 **device, block number, size** 로 변환하여 `get_block`을 호출하고 버퍼 캐시에 해당 블록이 있는지 검색합니다.  
3. 만약 buffer cache에 이미 캐시된 블락이 있다면(**Cache Hit**) VFS에 바로 반환합니다.
    - 요청한 블록이 캐시에 없으면 파일 시스템에게 블록을 요청하고, 그 블록을 **자신의 cache entry에 추가** 한 다음에 VFS에게 반환합니다.
4. VFS는 애플리케이션에서 인자로 넘겨준 buf에 읽어온 블록을 복사하여 읽은 byte만큼 반환합니다. 

이제 write 진행 순서도 확인해보겠습니다. write는 바로 디스크에 쓰지 않는다는 것을 알 수 있습니다. <br>
![lec3_6](/assets/img/posts/koreauniv-system-programming/Lec3_6.png) <br>

1. 애플리케이션에서 write를 호출합니다.
2. VFS는 인자를 **device, block number, size** 변환하여 buffer cache의 block에 write를 수생합니다. 
    - 사용하고자 하는 block이 buffer cache에 없다면 이전의 read처럼 쓰고자 하는 블록을 먼저 디스크에서 가져옵니다.
3. buffer cahce의 블락에 write 작업을 수행하고 **dirty bit를 set** 합니다. **kworker** 는 주기적으로 dirty bit가 set 되어 있는 블록을 찾아 변경된 내용이 디스크에 반영될 수 있도록 합니다.

**kworker**: Buffer cache의 내용과 disk block의 **동기화** 를 수행하는 kernel thread입니다. 버퍼 캐시 엔트리 중에서 **dirty bit** 가 체크 되어 있는 블락들을 디스크에 저장합니다. **주기적** 으로 동작하며 데이터 블락보다 메타데이터 블락에 대해 더 자주 수행합니다. 동작 조건에는 block이 캐시에 머문 시간, dirty bit 체크와 같은 조건, 메모리 부족 등이 있습니다. 

또한 애플리케이션이 **fsync(2)** 시스템콜을 호출하여 강제 동기화를 요청하면 kworker는 강제적으로 버퍼 캐시의 내용을 disk에 반영합니다. Buffer Cache를 사용하는 시스템에서는 read/write에 대한 operation이 실제 디스크에 바로 반영된다고 보장하지는 않습니다. 이것이 DBMS와의 차이입니다. 

## Memory-Mapped File
파일을 **process address space의 한 부분** 으로 매핑하는 것입니다. open, read, write가 아니라 `mmap` 같은 함수로 파일을 열면 그냥 메모리 작업을 하듯이 파일에 대한 read, write 작업을 수행할 수 있습니다. 커널은 **memory mapped file** 에 대한 연산을 적절히 read, write로 변환하여 수행합니다. `Virtual Memory`와 `File System`의 통합으로도 볼 수 있습니다. 

장점으로는 사용자가 파일에 대한 I/O를 **memory access** 로 단순화 할 수 있다, 와 여러 개의 프로세스에서 파일을 open하여 사용하는 경우 커널은 동일한 파일에 대한 2개의 복사본을 유지할 필요가 없다 가 있습니다. Memory-Mapped File이 아닌 전통적인 방식으로는 다음과 같이 작동합니다. <br>

![lec3_7](/assets/img/posts/koreauniv-system-programming/Lec3_7.png) <br>
Read 작어블 한다고 하면, 디스크 -> 버퍼 캐시 (커널), 버퍼 캐시 -> 사용자 버퍼, 이렇게 **두번의 복사** 가 발생합니다. 버퍼 캐시는 커널 영역에 있기 때문에 애플리케이션이 바로 사용을 할 수가 없습니다. 이제 Memory-Mapped File의 동작 방식을 살펴봅시다. <br>

![lec3_8](/assets/img/posts/koreauniv-system-programming/Lec3_8.png) <br>

mmap을 호출하게 되면 버퍼 캐시를 사용하지 않고 파일이 **process address space에 attach** 하게 됩니다. 전통적 방식과 달리 디스크 -> Phyical Memory의 복사가 한번만 발생하게 되며, 프로세스가 물리 메모리를 직접 사용하게 됩니다. 이 방식을 사용하면 두 프로세스가 같은 파일을 사용하더라도 전통적 방식과 달리 파일이 매핑된 물리 메모리를 같이 공유하면 되기에 메모리도 아낄 수 있습니다. mmap을 통해 `메모리`와 `파일 시스템`이 통합되었다고 할 수도 있습니다.

### COW(Copy-on-write)
![lec3_9](/assets/img/posts/koreauniv-system-programming/Lec3_9.png) <br>
위 스냅샷은 여러 파일이 mmap으로 파일에 접근할 때의 모습입니다. 프로세스가 같은 물리 메모리를 공유하고 있는 것을 볼 수 있습니다. 이 메모리의 초기 상태는 **read-only 공유** 입니다. 이때 프로세스 A가 쓰기 작업을 시도하면 `Write Fault`가 발생하고 커널이 개입을 합니다. 커널은 페이지 복사본(**스냅샷**)을 생성하고, 프로세스 A만 새 페이지를 사용하게 됩니다. fork, container에도 COW가 사용되며, 각 프로세스나 컨테이너가 실제 수정을 시도할 때 복사를 하는 방식으로 메모리를 절약할 수 있습니다. 

## 리눅스 파일 시스템
주요 파일 시스템에는 ext4(리눅스 기본, Extent 방식, 저널링), XFS(대용량 파일 지원), Btrfs(스냅샷, 압축 기능) 등이 있습니다. 강의에서는 Ext4에 대해 더 알아보도록 하겠습니다.

### Ext4
![lec3_10](/assets/img/posts/koreauniv-system-programming/Lec3_10.png) <br>
**Extent** 는 연속된 데이터 블록의 묶음이라고 생각하면 됩니다. 기존의 개별 블록 방식으로는 각 블록에 대한 포인터가 필요했다면, extent는 **파일 블록 번호, 물리블록 시작번호, 길이** 로 구성되어 연속된 블록들을 나타냅니다. Extent의 크기는 최대 128MB로 블록 사이즈가 4KB, extent 별 최대 블록 개수가 32768이라면 Max extent size는 4KB * 32768 = 128MB가 됩니다. 

extent의 장점으로는 
1. 메타데이터 감소
2. 성능 향상(디스크 이동 최소화, sequential read 최적화 등)
이 있습니다.

### 저널링
변경 내용을 **저널(journal)** 에 기록하여 크래시에 대응하는 방법입니다.

저널 영역(log)이라는 별도의 디스크 공간을 사용하여 
1. 저널에 변경 내용을 기록하고
2. 실제 파일 시스템을 업데이트 (checkpointing) 합니다.
3. 그 후 저널에 **commit** 표시를 합니다. 저널에 기록이 완료되었으니 파일 시스템에 업데이트 해도 된다는 뜻입니다. (강의 자료 순서가 이상한가..)
4. 만약 장애가 발생했을 경우 저널을 읽어 Committed transaction을 재실행 합니다.

저널링 모드에는 3가지 모드가 있습니다: Ordered, Journal, Writeback
### Ordered (default)
메타데이터만 저널링 하고 데이터를 먼저 쓰는 방식입니다. 

1. 디스크에 data write
2. 저널에 메타데이터 기록
3. 커밋
4. 메타데이터를 파일 시스템에 쓰기

순서로 진행합니다. 성능과 안전성이 균형을 이루나 데이터는 보장이 안된다는 단점이 있습니다.

### Journal
메타데이터와 데이터를 모두 저널링 합니다.

1. 저널에 데이터 기록
2. 저널에 메타데이터 기록
3. 커밋
4. 데이터랑 메타데이터를 파일 시스템에 쓰기 

최고의 안전성을 지니지만 느립니다.

### Writeback
메타데이터만 저널링을 합니다.

1. 저널에 메타데이터 기록
2. 커밋
3. 파일 시스템에 메타데이터 쓰기
4. 디스크에 데이터 쓰기

가장 빠르지만 데이터 손실 가능성이 있습니다. 

# Lecture 4
## Disk Structure
![lec4_1](/assets/img/posts/koreauniv-system-programming/Lec4_1.png) <br>

위 그림은 하드 디스크의 물리적 구조를 보여주고 있습니다. 주요 구성 요소는 다음과 같습니다.
- Platter (플래터): 데이터가 저장되는 원판
- Spindle (스핀들): 플래터를 회전시키는 축
- Track (트랙): 플래터의 동심원 형태로 데이터가 저장되는 경로
- Sector (섹터): 트랙을 나눈 최소 저장 단위
- Cylinder (실린더): 여러 플래터에서 같은 위치의 트랙들의 집합
- Arm assembly (암 어셈블리): 헤드를 이동시키는 기계 장치
- Read-write head (읽기-쓰기 헤드): 실제로 데이터를 읽고 쓰는 장치

## Disk Scheduling
디스크 스케줄링은 애플리케이션의 디스크에 대한 작업 요청을 스케줄링하여 보다 빠른 접근 시간과 디스크에 대한 bandwidth를 제공하는 것을 목표로 하고 있습니다. 하드웨어에 대한 효율적인 사용을 운영체제가 맡는 것입니다.

디스크의 접근 시간 **Access Time** 은 seek time + rotation delay + transfer delay로 계산 가능합니다.

```
블록들을 아래의 순서로 쓴다고 할 때, 소요되는 시간을 계산하시오. 디스크는 100개의 실린더를 가
지고 블록 크기는 4KB이며 또한, 디스크 스케줄링은 C-SCAN을 사용한다. 처음 디스크 헤더는 실린
더 50 의 위치에 있다고 가정하고, I/O 버스의 속도는 32비트 33MHz 이다. 실린더 1개를 이동하는데
걸리는 시간은 0.5ms 이고, rotation delay 는 무시하시오. RAID 0에서 쓰기를 할 때는 처음 블록은 디
스크1에, 다음 블록은 디스크2에 쓰고 이를 반복한다..
20 60 78 12 55 34 81 45

2) RAID 0 사용 시 시간 계산
핵심 아이디어: 두 디스크가 병렬로 동작하므로, 더 오래 걸리는 디스크의 시간이 전체 시간입니다.
데이터 분배:

디스크1: 20, 60, 78, 12, 55, 34, 81, 45 중 홀수 번째 → 20, 78, 55, 81
디스크2: 짝수 번째 → 60, 12, 34, 45

C-SCAN 스케줄링 (현재 위치 50에서 시작)

50에서 끝(99)까지 오름차순으로 이동 → 다시 처음(0)에서 끝까지

디스크1 접근 순서: 55 → 78 → 81 → (끝) → 20

seek time = (55-50) + (78-55) + (81-78) + (99-81) + (99-0) + (20-0) = 5+23+3+18+99+20 = 168 실린더
시간 = 168 × 0.5ms = 84ms
transfer time = 4블록 × 0.03ms = 0.12ms
소계: 84.12ms

디스크2 접근 순서: 60 → (끝) → 12 → 34 → 45

seek time = (60-50) + (99-60) + (99-0) + (12-0) + (34-12) + (45-34) = 10+39+99+12+22+11 = 193 실린더
시간 = 193 × 0.5ms = 96.5ms
transfer time = 4블록 × 0.03ms = 0.12ms
소계: 96.62ms

전체 시간 = max(84.12ms, 96.62ms) = 96.62ms (두 디스크가 동시에 작동하므로 더 긴 쪽 기준)

3) 디스크 1개 사용 시
접근 순서: 20 → 60 → 78 → 12 → 55 → 34 → 81 → 45 (C-SCAN)

정렬하면: 50에서 시작 → 55, 60, 78, 81(끝) → 0, 12, 20, 34, 45
seek time = (55-50) + (60-55) + (78-60) + (81-78) + (99-81) + (99-0) + (12-0) + (20-12) + (34-20) + (45-34)
= 5+5+18+3+18+99+12+8+14+11 = 193 실린더
시간 = 193 × 0.5ms = 96.5ms
transfer time = 8블록 × 0.03ms = 0.24ms
소계: 96.74ms


성능 비교

RAID 0: 96.62ms
디스크 1개: 96.74ms
향상도: 0.12ms 개선 (약 0.12%)
```

- **Seek time**: 디스크 암이 찾고자 하는 섹터가 위치한 목표 실린더로 헤드가 이동하는 시간, 탐색 시간입니다.
- **Rotation delay**: 디스크가 회전하며 요구되는 섹터를 헤드에 위치시키는데 걸리는 시간, 회전 지연 시간입니다.
- **transfer delay**: 디스크가 읽어온 데이터를 운영체제에게 전달하는데 걸리는 시간입니다.

이 중 disk scheduling을 통해서 줄이고 싶은 시간은 **seek time** 입니다. 회전 지연 시간은 제조사의 몫입니다. 

## Disk Scheduling - 큐 관리
![lec4_2](/assets/img/posts/koreauniv-system-programming/Lec4_2.png) <br>

Disk controller는 디스크 작업에 대한 요청을 저장해 두는 queue를 관리합니다. queue에 들어온 작업의 순서를 재배치함으로써 작업에 필요한 응답 시간(**seek time**)을 줄일 수 있습니다. 각 번호는 실린더 번호를 의미합니다. 디스크 스케줄링도 프로세스 스케줄링처럼 다양한 기법이 존재하는데 하나씩 살펴보겠습니다.

### FCFS
![lec4_3](/assets/img/posts/koreauniv-system-programming/Lec4_3.png) <br>
Disk 작업을 요청된 순서대로 처리하는 방법입니다. 본질적으로 공평하다만 가장 빠른 서비스를 제공하지는 못합니다. 위 예시에서 헤드의 이동거리는 (98-53) + (183-98) + (199-183) + (199-37) + (122-37) + (122-14) + (124 – 14) + (124 – 65) + (67 – 65) = `672`입니다.

### SSTF(Shorteset-Seek-Time-First)
![lec4_4](/assets/img/posts/koreauniv-system-programming/Lec4_4.png) <br>
현재 disk의 헤드 위치에서 가장 가까운 실린더에 대한 요청을 처리합니다. Disk 이동거리 측면에 대한 효율 측면에서는 유리합니다. 

그러나 일부 작업에 대해서 기아 상태(starvation)가 발생할 수 있다는 문제가 있습니다. 위 그림을 예시로 들면 14 -> 186 순서로 처리하려는 과정에서 14의 작업이 처리되던 중간에 15, 20, 30과 같은 요청이 들어오면 186에 대한 요청은 기아 상태에 빠져버립니다. 

### SCAN
![lec4_5](/assets/img/posts/koreauniv-system-programming/Lec4_5.png) <br>
![lec4_6](/assets/img/posts/koreauniv-system-programming/Lec4_6.png) <br>
Disk의 작업 요청을 disk의 한 쪽 끝에서 반대쪽 끝으로 이동하면서 처리합니다. 한쪽 방향으로 쭉 가면서 처리하고 끝에 도달하면 반대 방향으로 스캔하는 방식입니다. SCAN 방식은 SSTR에서 발생하는 기아 상태를 해결할 수 있고, FCFS의 응답 시간 문제도 어느 정도 줄일 수 있습니다. 

그러나 진행 방향의 뒤쪽 실린더에 대한 요청이 들어오면 방향이 반대로 바뀌기 전까지는 그 요청을 처리하지 못하는 **Fairness** 문제가 발생합니다. 대부분의 요청에 대한 일정한 대기 시간을 보장하지 못합니다. 대기 시간의 분산(Variance)가 큽니다.

### C-SCAN
![lec4_7](/assets/img/posts/koreauniv-system-programming/Lec4_7.png) <br>
SCAN 방식을 개선한 방식으로 arm의 이동방향을 한쪽 방향으로만 수행합니다. 끝에 도달하면 다시 처음으로 점프합니다. SCAN에서 문제였던 job에 대한 대기시간을 어느 정도 uniform하게 바꿀 수 있습니다. 그러나 **실제 요청이 없어도 끝까지 가야 한다는 비효율적** 이다는 문제가 존재합니다.

### C-LOOK
![lec4_8](/assets/img/posts/koreauniv-system-programming/Lec4_8.png) <br>
끝까지 않고 **마지막 요청이 있는 실린더까지만** 이동합니다. C-SCAN보다 더 효율적입니다. 

## Disk Scheduling 알고리즘의 선택
Disk Scheduling의 성능에 영향을 주는 요인에는 다음과 같은 것들이 있습니다.
- 요청된 disk 작업의 개수
- 요청된 작업이 수행될 실린더의 위치
- File allocation 방법 => 연속 할당이 Linked/Indexed 할당보다 헤드 이동이 적어서 유리할 수 있음
- Directory, index block의 위치 => File을 open할 때마다 디스크에 저장된 directory 정보와 file의 data 정보를 가져와야 합니다. 적절한 directory entry 위치와 file data block의 위치는 성능을 높일 수 있고, 앞에서 다룬 buffer cache 역시 성능을 높일 수 있는 예 중 하나입니다.

Disk Scheduling을 선택할 때 정답은 없습니다. 앞에서 봤던 알고리즘들은 **실린더 위치 최적화** 에 초점을 두었고, latency를 어떻게 우선순위화할 것인가? 라는 관점도 있습니다. 권장 사항은 장치의 특성, 파일 시스템의 allocation 방식에 따라 유리한 알고리즘을 선택하는 것입니다.

## Fast File System(`84)
기존 UFS(UNIX File System)의 문제점에는 
- Low Throughput: 블록 크기가 512바이트로 너무 작고 Disk bandwidth의 2퍼센트 정도 밖에 못 씀
- Long Seek Time: Data block들이 임의로 배치되고 Inode와 Data block 사이의 거리가 멀리 떨어져 있음
등이 있습니다.

Fast File System의 개선점에는 다음이 있습니다.
- 블록 사이즈를 4KB나 8KB로 설정하여 **Throughput** 을 높였습니다.
- Fragmentation으로 공간 효율성을 높였습니다 -> 큰 사이즈의 블록으로 인해 낭비되는 공간을 fragmentation으로 해결
- 한 cylinder group에 **superblock, inode, files** 를 함께 배치하고, 같은 디렉토리의 파일과 inode를 같은 그룹에 할당하는 방식으로 Seek time을 감소시켰습니다.

이 결과 UFS 대비 Bandwidth가 15배 증가하는 성능 개선(read, write)이 가능했습니다.

# Lecture 5
![lec5_1](/assets/img/posts/koreauniv-system-programming/Lec5_1.png) <br>

위 그림은 Driver Kernel Interface의 역할을 보여주고 있습니다. 커널과 드라이버 사이의 `표준 인터페이스`로써 다양한 하드웨어를 통일된 방식으로 다룰 수 있게 해줍니다. 아래 그림은 DKI 계층 구조를 나타냅니다. <br>

![lec5_2](/assets/img/posts/koreauniv-system-programming/Lec5_2.png) <br>

- kernel: 운영체제 핵심을 의미하는 계층입니다
- DKI: **표준 인터페이스** 로 모든 드라이버가 따라야 하는 규약을 나타냅니다.
- Device Drivers: 각 장치별 소프트웨어 드라이버
- Device Controllers: 하드웨어 컨트롤러
- Physical Devices: 실제 물리적 장치 

## Driver Kernel Interface
DKI는 다양한 I/O 장치들을 일관되게 다루기 위한 Kernel과 Driver 사이의 Interface를 의미합니다. 이를 통해 어플리케이션이 디바이스의 종류에 무관하게 파일에 접근할 수 있고, 새로운 디스크나 장치가 추가되어도 OS는 **동일한 인터페이스** 로 인식할 수 있습니다. 또한 I/O 시스템이 하드웨어와 분리됨으로써 OS 개발자나 하드웨어 제조사가 **독립적으로** 작업할 수 있습니다. 

각 OS마다 서로 다른 DKI를 정의하기 때문에(Windows: HAL-Hardware Abstraction Layer. Linx: loadable module) 제조사는 그에 맞춰 OS마다 다른 Device Driver를 작성해야 합니다.

### Driver Kernel Interface 설계
**Device as file** => `/dev/tty0`, Unix/Linux에서 모든 장치를 파일처럼 다룹니다. `/dev` 디렉토리에 장치 파일들이 존재합니다.

Layering(계층화)을 통해 device와 driver를 구분하여 역할을 명확히 합니다.
- device: 디바이스(하드웨어)내의 **펌웨어** 를 의미합니다. 하드웨어 제조사가 정의하여 직접 하드웨어를 제어하는 프로그램이고, Device driver가 이를 이용하여 장치를 제어합니다.
- driver: **커널 모듈** 로 삽입되는 device driver입니다. 삽입될 OS의 DKI가 정의하는 동작을 device에 맞춰 구현합니다. 

장치를 식별하는 번호 체계는 Major Number, Minor Number로 구성됩니다.
- Major Number: Device Driver를 식별합니다
- Minor Number: 특정 device를 식별합니다. 

### Driver Kernel Interface: mechanisms
**Abstraction(추상화)** => 장치들을 `추상화`하여 구분합니다. 
- Character and Block device
- Sequential and Random access
- Synchronous and Asynchronous data transfer
- Read-only, Write-only, Read/Write
- 이 외에도 공유 가능 여부, 장치 속도 등에 따라 구분할 수 있습니다.

**Encapsulation** => 디바이스 드라이버 operation을 공통화 하는 것입니다. 각 장치를 제어하는 연산들을 캡슐화하여 표준 인터페이스는 따르되 내부 구현은 각 장치의 특징에 따라 다르게 설계하도록 하는 방식입니다. (dev_open, dev_read, dev_intr..)

### character Device
![lec5_3](/assets/img/posts/koreauniv-system-programming/Lec5_3.png) <br>
데이터를 버퍼에 모으지 않고 즉시 처리합니다(**Unbuffered** access). 그리고 파일 시스템을 우회하여 블록 단위가 아니라 **원시(raw)** 데이터에 접근을 합니다. 디스크의 경우 File System 지원 없이 직접 접근하는 것입니다. 장치에 대한 입출력 단위는 가변적이며 open, read, write, close 등의 시스템 콜을 필요로 합니다. 키보드, 마우스, 시리얼 포트, `터미널(tty)` 등이 있습니다.

### Block Device
![lec5_4](/assets/img/posts/koreauniv-system-programming/Lec5_4.png) <br>
데이터를 버퍼에 모아서 처리합니다(**Buffered** access). 그리고 고정 길이 블록 단위로 작동합니다. 보통 512 바이트, 4KB 등 고정된 크기가 있습니다. 대표적인 장치는 디스크로 파일 시스템이 블록 디바이스에 대한 입출력을 담당합니다. 즉, 파일 시스템을 통해서 block device에 있는 데이터에 접근한다는 뜻입니다. 

### sysfs
`/sys` 디렉토리는 일종의 가상 파일 시스템(pseudo file system)입니다. 실제 파일이 아닌 커널 정보를 파일처럼 표현하는 것입니다. 장치/드라이버 속성을 커널에서 유저 공간으로 노출시키기 위해 사용합니다. 예를 들어 `/sys/block/sda/size`는 디스크 크기를 나타냅니다. 

**Device Tree** 개념을 사용하는데, 이는 하드웨어의 계층 구조를 트리로 표현한 것으로, Bus와 Device Class 같은 하드웨어 분류 체계를 지원합니다. 또한 **hotplug**를 지원하는데, Hotplug는 시스템 실행 중에 장치를 추가하거나 제거할 수 있는 기능으로, **udev 데몬** 과 협력하여 동작합니다. USB를 꽂으면 커널이 uevent를 발생시키고, udev가 /sys를 확인한 후 /dev에 장치 노드를 자동으로 생성합니다.

`/proc/<pid>`는 프로세스 정보를, `/sys`는 하드웨어/드라이버 정보를 나타내는데, 서로 보완적인 관계라고 볼 수 있습니다.

### Applicatoin, Device and Kernel
애플리케이션의 관점에서는 Character device든 Block device든 모두 파일처럼 다룰 수 있어야 합니다. Linux에서 device에 대한 시멘틱은 **파일로 추상화** 되며, 모든 디바이스에 대해서는 파일에 대한 **단일화된 interface** 를 통해서 접근할 수 있습니다.(ex: VFS)

Device Driver는 **DKI** 에서 정의된 인터페이스대로 device driver를 구현한 다음, device specific 한 부분들은 드라이버 내부에서 처리하도록 만들어두비다. 이를 통해 커널과 애플리케이션은 장치의 특성에 관계없이 device driver를 통해 장치를 제어할 수 있게 됩니다. 

## RAID
RAID: Redundant arrays of independent disks, 처음에는 저렴한(Inexpensive) 디스크들의 배열이었으나 디스크의 크기가 작아지고 가격이 저렴해지면서 많은 수의 디스크를 장착할 수 있게 되었고, **독립적인(Independent)** 로 바뀌었습니다. RAID의 장점으로는 다음 항목들이 있습니다.
- Reliability: `ECC(Error Correcting Code)`를 여러 디스크에 분산, 저장하여 데이터의 손실을 방지할 수 있습니다.
- Performance: 여러 디스크가 **parallel** 하게 동작함으로써 여러 디스크에서 동시에 읽기/쓰기가 가능해지기 때문에 향상된 data transfer bandwidth를 제공할 수 있습니다.

### RAID 0
![Lec5_5](/assets/img/posts/koreauniv-system-programming/Lec5_5.png) <br>
여러 디스크를 하나로 묶은 것입니다(**strip**). Redundancy(중복 저장)와 parity bit(오류 검출 코드)를 제공하지 않습니다. 그림을 보면 디스크 두개에 데이터가 나뉘어져 있는 것을 볼 수 있는데, **두 개의 디스크를 하나로 쓰는 것** 이라고 생각하면 됩니다. 두 디스크에서 동시에 읽기/쓰기를 하고 용량 낭비가 없다는 점에서 장점이 있으나, Disk0이 고장나면 Disk1의 데이터도 쓸모가 없어지는 **신뢰성** 문제가 큽니다. 

### RAID 1
![Lec5_6](/assets/img/posts/koreauniv-system-programming/Lec5_6.png) <br>
두 개의 디스크에 같은 내용을 **복사(Mirroring)** 하여 저장합니다. 확보할 수 있는 공간은 n개의 디스크에서 n/2로 줄어듭니다. 디스크 한쪽에서 fail이 발생해도 다른 디스크의 내용을 이용하여 복구할 수 있다는 점에서 **Reliability** 가 향상되었고, 두 디스크에서 분산 읽기가 가능하다는 점에서 읽기 성능이 좋아지는 장점이 있지만, Disk0의 일부 비트가 오류로 잘못 저장되면 Disk0, Disk1 **어느 것이 올바른지** 알 수가 없게됩니다. 

### RAID 2
![lec5_7](/assets/img/posts/koreauniv-system-programming/Lec5_7.png) <br>
비트 단위로 데이터를 여러 디스크에 분산 저장하는 방식입니다(**Bit-level Striping**). 그리고 에러 체크와 복구를 위해 **humming code parity** 를 사용합니다. 그림을 보면 4개의 디스크를 묶어서 하나의 디스크에 쓰는 것처럼 데이터를 기록하고, 패리티를 저장한느 별도의 디스크를 통해 데이터가 소실되었을 때 복구합니다. 4 data bit마다 3 parity bit가 필요해 너무 비효율적이다는 단점이 있습니다. 

### RAID 3
![lec5_8](/assets/img/posts/koreauniv-system-programming/Lec5_8.png) <br>
**byte-level strip** 즉 바이트 단위로 데이터를 분산 저장합니다. **XOR ECC(error-correcting) codes** 라는 더 간단한 패리티를 사용하며 별도의 디스크에 패리티를 저장합니다. 그림에서는 Disk0, Disk1, Disk2의 한 바이트마다 Disk 3에 XOR ECC를 기록하고 있습니다. 계산은 Disk3 = Disk0 ^ (Disk1 ^ Disk2) 이런 식으로 합니다. 

패리티 디스크가 하나만 필요하여 저장 효율이 개선되었으나, 바이트 단위로 저장하기에 성능이 떨어지고, Parity disk 접근에 병목이 발생할 수 있으며 Parity disk가 고장나면 복구할 수가 없다는 단점이 있습니다. 

### RAID4
![lec5_9](/assets/img/posts/koreauniv-system-programming/Lec5_9.png) <br>
**Block level strip** 즉 블록 단위로 데이터를 저장하며 똑같이 XOR ECC 패리티를 사용합니다. RAID3에 비해 저장 단위가 커져 성능이 향상되었으나 여전히 Parity 디스크 병목, Parity 디스크 고장 시 복구 불가능이라는 단점이 남아있습니다.

### RAID5
![lec5_10](/assets/img/posts/koreauniv-system-programming/Lec5_10.png) <br>
RAID4와 비슷하나 parity disk를 별도로 쓰는 대신 **데이터 디스크에 parity를 분산 저장** 합니다. 그림을 보면 첫 번째 줄부터 네 번째 줄까지 패리티가 저장된 디스크가 다른 것을 볼 수 있습니다. 이 방식을 사용하면 parity 디스크 병목 문제를 해결할 수 있으며 Parity 파손에 따른 손실을 최소화할 수 있어 **Reliability** 가 우수합니다. 

# Lecture 7: Flash Memory

## Flash Memory Features
플래시 메모리는 **EEPROM(Electrically Erasable PROM)**의 일종으로, 전기적으로 데이터를 지울 수 있는 비휘발성 메모리입니다. 메모리의 발전 과정을 살펴보면 다음과 같습니다.

**메모리 발전 과정:**
- **ROM(Read-Only Memory):** 제조 시 게이트 연결을 고정시켜 읽기만 가능하도록 만든 메모리
- **PROM(Programmable ROM):** 사용자가 한 번 프로그래밍 할 수 있는 ROM. 펌웨어 저장소로 주로 사용
- **EPROM(Erasable PROM):** 
  - UV-EPROM은 자외선으로 지울 수 있음
  - EEPROM은 전기적으로 지울 수 있음
- **플래시 메모리:** EEPROM의 발전형으로, 일부 데이터를 선택적으로 지울 수 있게 됨

**플래시 메모리의 장점:**
- 빠른 접근 속도(특히 읽기)
- 저전력 소비
- 비휘발성(전원이 꺼져도 데이터 유지)

**저장 단위 계층:**
- **셀(Cell):** 비트 하나를 저장하는 최소 단위
- **페이지(Page):** 여러 셀이 모여 구성
- **블록(Block):** 여러 페이지가 모여 구성

**Bad Block:** 제조 과정이나 사용 중에 문제가 발생한 블록을 표시하여 사용하지 않도록 하는 것

## Flash Memory Types

플래시 메모리는 크게 NOR 플래시와 NAND 플래시로 나뉩니다.

### NOR 플래시
셀을 병렬로 연결한 구조로, 다음과 같은 특징이 있습니다.
- 랜덤하게 데이터를 읽기/쓰기 가능(바이트 단위)
- RAM에 근접하는 읽기 속도
- 쓰기는 속도가 느려서 주로 읽기 전용으로 사용
- 지우기는 블록 단위로 수행
- **XIP(eXecute In Place):** 프로세서가 NOR 플래시로부터 직접 코드를 가져와서 실행하는 기능

### NAND 플래시
셀을 직렬로 연결한 구조로, 현재 가장 널리 사용되는 플래시 메모리입니다.
- 페이지 단위로 읽기/쓰기 가능
- 가격이 저렴하여 대용량 저장에 적합
- 데이터 저장장치로 주로 사용됨
- NOR 플래시에 비해 쓰기, 지우기 연산이 빠름
- **셀당 저장 비트수에 따른 분류:**
  - SLC(Single Level Cell): 1비트
  - MLC(Multi Level Cell): 2비트
  - TLC(Triple Level Cell): 3비트
  - QLC(Quad Level Cell): 4비트

## NAND Flash Memory 구조

NAND 플래시는 계층적 구조를 가지고 있습니다. <br>

![lec7_1](/assets/img/posts/koreauniv-system-programming/Lec7_1.png) <br>

**구조 예시:**
```
1 Device = 8192 Blocks (128 MB)
1 Block = 32 pages (sectors)
1 Page = (512 + 16) Bytes
```

**각 페이지의 구성:**
- **Main Array(512B):** 실제 데이터 저장 영역
- **Spare Array(16B):** ECC(데이터 정정용 코드), 배드 블록 표시 등에 사용

플래시 메모리의 다양한 스펙(블록 크기: 256KB~16MB, 페이지 크기: 2K~16KB)이 존재하며, 제조사와 세대에 따라 다릅니다.

## Flash Memory의 단점

플래시 메모리는 여러 장점에도 불구하고 치명적인 단점들이 있습니다.

**주요 단점:**
- **쓰기가 느림:** 읽기에 비해 쓰기 속도가 현저히 느림
- **덮어쓰기 불가능:** 데이터를 덮어쓰기 전에 반드시 지우기(erase)가 선행되어야 함
- **지우기 시간이 김:** 지우기 연산이 읽기/쓰기보다 훨씬 긴 시간 소요
- **지우기 단위가 블록:** 작은 단위 수정도 블록 전체를 지워야 함
- **블록의 지움 횟수 제한:** 일반적으로 10,000~100,000번. 이는 플래시가 하드디스크를 완전히 대체하지 못하는 주요 이유

**Wear-leveling의 필요성:**
플래시의 aging 현상을 완화하기 위해 모든 블록이 균등하게 사용되도록 관리하는 기법이 필요합니다. Erase하면 모든 비트가 1로 초기화됩니다.

## Flash Memory Erasing Process

플래시 메모리에서 데이터를 쓰는 과정은 복잡합니다.

**Garbage Collection(GC) 과정:**
1. 지울 블록 선택
2. 해당 블록 내의 유효 데이터를 다른 곳으로 보관
3. 선택된 블록을 지움

**디스크와의 차이점:**
- **디스크:** Overwrite가 가능하여 그 자리에 바로 쓸 수 있음
- **플래시:** Overwrite가 불가능. 기존 데이터를 옮긴 후 지우고 다시 써야 함

**발생하는 문제들:**
1. **버전 관리 문제:** 데이터를 옮긴 후 새로 쓰면, 이전 위치의 데이터는 **stale**(오래된, 무효한) 상태가 됨. 이를 지워야 할지 냅둬야 할지 판단 필요
2. **GC 성능:** 예전에는 유효했지만 지금은 stale한 데이터를 찾아서 지운 다음 빈 공간을 계속 만들어야 하는데, 이 과정의 성능이 전체 시스템 성능에 큰 영향을 미침

## Flash vs. Disk 성능 비교

플래시와 디스크의 성능 특성은 매우 다릅니다.

**Latency 비교:**

| 구분 | Latency(읽기/쓰기) | Bandwidth |
|------|-------------------|-----------|
| 디스크 | 11ms / 11ms | 220 MB/s |
| 플래시 | 25μs / 0.3ms<br>6ms(지우기) | 100MB/16MB<br>per sec per die |

**주요 특징:**
- **디스크:** 읽기/쓰기 latency가 비슷함
- **플래시:** 읽기 latency는 엄청 빠르지만, 쓰기가 느리고, 지우기는 더 느림
- **Bandwidth:** Latency에 비해 상대적으로 느림(벌크로 읽는 게 느림)

**SSD(Solid State Drive):**
- 플래시 die 하나가 아니라 여러 개를 묶어서 사용
- **RAID** 아이디어를 적용: 여러 개를 동시에 읽고 쓰니 더 빠름
- 구성: 컨트롤러 + DRAM 캐시 + 여러 개의 NAND 플래시
- 컨트롤러에 여러 채널이 연결되고, 각 채널에 여러 다이를 연결하여 병렬 처리

## Flash & File System

플래시 메모리를 소프트웨어에서 사용하려면 파일 시스템을 올려야 하는데, 두 가지 접근 방식이 있습니다.

### 1. 기존 블록 기반 파일 시스템 사용 (예: Ext4 + FTL)

파일 시스템 계층에서는 플래시 메모리를 디스크처럼 사용합니다. 디스크와 플래시 메모리의 **mismatch**를 해결하기 위해 **FTL(Flash Translation Layer)**를 사용합니다.

**장점:**
- 기존 파일 시스템을 그대로 사용 가능
- 플래시 디바이스에 대한 고민 없이 편하게 사용
- Adaptation은 제조사가 제공

**단점:**
- FTL의 오버헤드
- 플래시 특성을 완전히 활용하지 못함

### 2. 플래시 파일 시스템 (예: F2FS)

플래시 특성에 맞게 설계된 파일 시스템입니다.

**특징:**
- FTL 없이 플래시를 raw device처럼 직접 다룸
- Wear-leveling을 파일 시스템 레벨에서 관리
- inode, directory 외에 inode map, checkpoint 등 추가 자료 구조 사용
- Log-structured 접근 방식 사용

**주요 이슈:**
- Stale 공간의 복원을 위한 garbage collection 수행
- 이 오버헤드를 어떻게 최소화할 수 있는지가 핵심

**사례:** F2FS, JFFS2, YAFFS, UBIFS (주로 임베디드 시스템에 적용)

## FTL (Flash Translation Layer)

플래시를 디스크처럼 사용하기 위한 변환 계층입니다. <br>
![lec7_2](/assets/img/posts/koreauniv-system-programming/Lec7_2.png)<br>

### FTL Mechanism

FTL의 동작 방식은 다음과 같습니다.

**1. Write 시:**
- 새로운 플래시 페이지 할당
- **Wear-leveling**을 고려한 페이지 할당 (특정 블록만 계속 쓰지 않도록 분산)
- 파일 블록을 플래시 페이지(블록 포함)로 매핑
- 과거 페이지는 **"stale"**(무효) 상태가 됨

**2. 매핑 테이블 갱신:**
- 새로운 페이지로 매핑 테이블 갱신
- FTL이 매핑 테이블을 유지 관리

**3. Garbage Collection:**
- Free 페이지의 수가 일정 수준 이하로 떨어지면
- Stale 페이지에 대하여 garbage collection 시작

**Versioning:**
복구를 위해 이전 버전을 유지하는 것이 도움이 되는 경우가 많습니다. 하지만 공간 효율과의 트레이드오프가 있습니다.

### FTL Sector Mapping
![lec7_3](/assets/img/posts/koreauniv-system-programming/Lec7_3.png) <br>

**용어:**
- **LSN(Logical Sector Number):** 파일 시스템이 사용하는 논리적 섹터 번호 (= File block number)
- **PSN(Physical Sector Number):** 실제 플래시의 물리적 페이지 번호 (= Page Number)

**동작 방식:**
파일 시스템은 기본적으로 논리 블록(LSN)과 물리 블록(PSN)을 매핑합니다. FTL은 이 매핑 테이블을 유지하며, 쓰기 요청이 오면 새로운 물리 페이지를 할당하고 테이블을 업데이트합니다.

**예시:**
"Write to LSN=9" 요청이 오면:
- 매핑 테이블에서 LSN 9를 확인
- 새로운 PSN(예: PSN=3)을 할당
- LSN 9 → PSN 3으로 매핑 테이블 업데이트
- 이전 PSN은 stale 상태가 됨

### FTL Block Mapping
![lec7_4](/assets/img/posts/koreauniv-system-programming/Lec7_4.png) <br>

페이지 단위가 아닌 블록 단위로 매핑하는 방식도 있습니다.

**용어:**
- **LBN(Logical Block Number)**
- **PBN(Physical Block Number)**

**동작 방식:**
"Write to LSN=9" 요청 시:
- LBN = 9/4 = 2 (블록 크기가 4 페이지라고 가정)
- Offset = 1
- 매핑 테이블에서 LBN 2 → PBN 1
- PBN 1의 Offset 1 위치에 쓰기

**장단점:**
- 매핑 테이블 크기가 작아짐
- 하지만 granularity가 커져서 유연성이 떨어짐

### FTL 구조

FTL의 위치는 정해지지 않았으며, 플래시 내부에 있을 수도 있고 커널이 대신 제공할 수도 있습니다. Software layer인 것은 확실합니다.

**계층 구조:**
```
File System
↓
Block Device Driver
↓
FTL (Flash Translation Layer)
  ├─ STL (Sector Translation) - Mapping, GC, Wear leveling
  ├─ BML (Block Management) - Bad Block Management, Error Handling
  └─ LLD (Low Level Driver) - Flash Interface
↓
H/W: NAND Flash Device (Raw NAND + NAND Controller)
```

## SSD (Solid State Drive)

SSD는 NAND 기반 저장장치로, 다음과 같은 특징이 있습니다.

**정의:** NAND 플래시를 기반으로 한 저장장치

**구성:** 
- 플래시 컨트롤러(FTL 포함)
- DRAM 캐시
- 여러 개의 NAND 플래시 다이

**장점:**
- 속도: HDD보다 수백~수천 배 빠름(마이크로초 단위)
- 무소음
- 내구성
- 저전력

**단점:**
- 쓰기 수명 제한
- **데이터 복구 어려움:** FTL이 손상되면 복구 불가능

**인터페이스:** SATA, NVMe(PCIe)

**파일 삭제 시 HDD와의 차이:**
1. SSD로 trim 명령 전송
2. SSD는 garbage collection으로 물리적으로 삭제(공간 확보)
3. HDD는 메타데이터만 지우고 실제 데이터는 남아있어 복구 가능하지만, SSD는 물리적으로 삭제되어 복구 불가

## 데이터센터 저장장치 계층 구조

데이터센터에서는 저장장치를 계층화하여 사용합니다.

| 계층 | 용도 | 대표 매체 | 특징 |
|------|------|-----------|------|
| **Tier 0 - 초고속 계층** | AI/ML, 실시간 트랜잭션 | NVMe SSD | 매우 낮은 지연시간, PCIe 4.0/5.0 인터페이스 |
| **Tier 1 - 고성능 계층** | 일반 DB, 가상화, 클라우드 스토리지 | SATA SSD | 빠르고 안정적, 비용은 HDD보다 높음 |
| **Tier 2 - 대용량 계층** | 미디어 파일 | HDD | 대용량, 저비용, 느림 |
| **Tier 3 - 아카이브 계층** | 비활성 데이터, 백업, 로그 | Cold HDD, Object Storage (S3) | 극저비용, 접근 지연 큼 |

### Storage Tiers: NVMe SSD vs SATA SSD vs Object Storage

**NVMe SSD:**
- Multiple I/O queues 지원
- 가장 빠른 성능

**SATA SSD:**
- SATA는 원래 HDD용 인터페이스
- NVMe보다는 느리지만 HDD보다 훨씬 빠름

**Object Storage (S3):**
- Key-value store 방식
- 파일 시스템이 아님 (no file, 평면 구조)
- Object = (ID=key, Data=value)
- Bucket: object의 모음

**Ext4(NVMe SSD)와 Amazon S3 비교:**

| 항목 | ext4 (NVMe SSD) | Amazon S3 |
|------|----------------|-----------|
| 접근 속도 | 10μs~1ms | 50~200ms |
| 속도 차이 | - | 약 1,000배 느림 |
| 전송 속도 | GB/s 단위 | MB/s 단위 |
| 접근 단위 | 바이트 단위 | 객체 단위 |
| 적합 용도 | 로컬 프로그램, DB | 백업, 로그, 대규모 AI 데이터 |
| 주요 장점 | 빠름 | 확장성, 안정성, 저비용 |

**More Comparison:**

| 항목 | ext4 (서랍) | S3 (창고) |
|------|-------------|-----------|
| 저장 위치 | 서버 로컬 SSD | 클라우드 스토리지<br>지역(리전) 무관 |
| 접근 방식 | OS 커널 I/O<br>(read(), mmap()) | HTTP/HTTPS API 기반<br>(GET) |
| 네트워크 | 없음 (로컬 버스) | 필수<br>(인터넷 / VPC 네트워크) |
| 처리단위 | 바이트 단위 | 객체 단위<br>작은 수정/부분 읽기 불가<br>객체 전체 다운로드 필요 |
| 확장성 | 제한적 | 수억 개 오브젝트 수용 가능<br>디렉토리 구조 아님 |
| 내구성 | 99.99% | 11 nines |

## File System for Flash

플래시를 위한 파일 시스템은 Log-structured 접근 방식을 기반으로 합니다.

### Flash 기반 전용 파일 시스템

플래시에 대한 새로운 자료 구조와 인터페이스를 도입합니다.
- inode, directory 외에 inode map, checkpoint 추가

**주요 이슈:**
- Stale 공간의 복원을 위해 garbage collection 수행
- 이 오버헤드를 어떻게 최소화할 수 있는지

**사례:** F2FS, JFFS2, YAFFS, UBIFS (임베디드 시스템에 적용)

### Log Structured File System 기반

Storage를 연속된 log로 처리하는 방식입니다. 원래 디스크 기반 파일 시스템으로 제안되었으나 플래시 파일 시스템의 기반이 되었습니다.

## Log Structured File System (LFS)

LFS는 1992년 Berkeley의 Sprite 운영체제 프로젝트의 일환으로 발표되었습니다.

### 핵심 아이디어

**Idea:** Write the aggregate of all the modifications sequentially (모든 수정사항을 한꺼번에 모아서 순차적으로 쓰자)

**Goal:** 
- Speed up file writing (파일 쓰기 속도 향상)
- Crash recovery (크래시 복구 간소화)

### Observation (관찰)

기존 파일 시스템의 문제점을 분석했습니다.

**Read 성능:**
- Memory caching에 의해 성능이 개선됨
- 문제 없음

**Write 성능의 문제:**
- Write는 caching 되었다가 디스크에 쓰게 됨
- Metadata가 나누어져 있어서 여러 개의 **small write**가 수행됨
- To update: data block, inode, directory, directory inode
- Data update는 asynchronous하게 이루어지지만 **metadata update는 synchronous**하게 이루어짐
- Metadata는 양이 많지 않음. Synchronous 해봤자 적음
- 결과: Linux file system은 디스크 bandwidth의 **5~10%만** 사용 for writing
- 이유: Small write하기 위해서 **seek**을 해야 하기 때문

**Crash Recovery의 문제:**
- Crash recovery 시 전체 디스크를 스캔하여 consistency 체크 필요
- Metadata update는 synchronous하지만 data update가 asynchronous하므로
- 시간이 너무 많이 걸림 (디스크 용량이 커짐에 따라 더욱 심각)

### LFS Idea: 한꺼번에 왕창왕창 쓰자

**핵심 개념:**

```
Linux File System: Write Request [1] [3] [5] [7] [9]
              ↓   ↓   ↓   ↓   ↓
         [1] [3] [5] [7] [9]  ← Seek Overhead

Log-structured FS: Write Request [1] [3] [5] [7] [9]
                        ↓
                 [1][7][9][3][5]  ← Sequential Write
```

**장점:**
1. Disk write(asynchronous)를 한 번에 "모아서" 처리
2. 모든 새로운 write를 파일에 append
   - Data, index information, directory 포함
3. 그래서 seek을 최소화
4. 한 번 쓸 때마다 큰 청크를 쓰니 효율이 올라감

**Crash Recovery:**
- **Append only** 방식이므로 파일의 제일 뒤에 있는 정보만 체크하면 됨
- Random하게 동작하는 경우가 많지 않음
- Crash 발생 시 바로 직전에 append 했던 부분까지만 safe하다고 보면 됨
- 디스크 전체를 다 뒤질 필요 없음

### LFS vs Linux FS 비교

**Linux File System:**
```
[DATA BLOCK] [INODE (데이터 블록에 대한 정보)] [INODE MAP (아이노드에 대한 정보)]
```
- Inode는 고정된 위치에 존재
- Update 시 여러 곳을 non-sequential하게 write

**Log-structured File System:**
```
[DATA BLOCK] [INODE] [INODE MAP] ─→ (Append)
```
- 모든 것을 sequential하게 append
- 원위치라는 개념이 없음

**예시: dir1/file1 and dir2/file2에 쓰기 동작**

Linux FS: 
- 8번의 non-sequential writes (①~⑧)
- 각각: dir1 inode, file1 data, file1 inode, dir1 directory, dir2 inode, file2 data, file2 inode, dir2 directory

LFS:
- 1번의 large sequential write
- 모든 것을 순서대로: file1 data → dir1 inode → dir1 directory → file2 data → dir2 inode → dir2 directory → inode map

### Structure on Disk

**Log: permanent structure**
- Sequential structure (논리적 개념)
- Disk에 존재하는 유일한 구조
- Read를 위한 indexing 정보 포함: inode

**Issues:**

1. **Read: 어떻게 indexing을 할 것인지**
   - 원위치가 없으니 read가 더 어려워짐
   - Write하면서 계속 append됨
   - Inode를 보고 위치를 찾아가야 하는데...

2. **빈 공간의 확보**
   - Append 함으로 시간이 지남에 따라 디스크 공간이 없어짐
   - Garbage collection 필요

### How to do "read"

**Using inode:**

Linux FS:
- Inode table이 디스크의 **고정된 위치**에 존재
- 파일에 번호를 할당 → 간단한 계산으로 inode 위치 찾기

LFS:
- Inode가 log 안에 위치 (고정 위치 없음)
- 새로운 자료 구조 **"inode map"** 도입
  - 각 inode의 current location을 유지
  - Inode map을 lookup하면 inode를 찾을 수 있음
  - Inode map은 **메모리에 caching**됨
- 항상 current inode를 가리키도록 함

### Free Space Management

**Goal:**
- Free space를 가능한 크게 유지하는 것
- 조각 내지 않고

**Possible Approaches:**
- **Threading:** 뜨개질하듯이 엮는 것 (old를 건너뛰며 저장)
- **Copying:** Live 데이터를 옮기는 것 (old를 같이 모음)

**LFS Approach: combination of threading and copying**
- Disk를 segments로 나눔 (512KB or 1MB)
- Segment가 쓰기의 단위
- 쓰기 전에 segment 내의 live 데이터는 다른 segment로 복사됨
  - 이를 **segment cleaning**이라고 함
- Segment들의 집합이 log임

### Threading and Copying 비교

**Threaded log:**
```
[old log end] ... [new log end]
     ↓                 ↓
[old][new][old][new][old][new]
  ↓    ↓    ↓
Old를 건너뛰며 저장
```

**Copy and Compact:**
```
[old log end] ... [new log end]
     ↓                 ↓
[old][old][old][new][new][new]
  ↘  ↓   ↙
Old를 같이 모음
```

### Segment Cleaning

**Three Steps:**
1. 몇 개의 segment(x)를 메모리로 읽음
2. Live data를 정리해서 모음
3. Disk segment로 씀: x보다 적은 개수의 segment로

**New Data Structure:**

**Segment Summary Block:**
- 각 log write의 정보를 포함
- File number, block number

**Version Number:**
- Inode map의 각 entry(file)에 대한 version number
- Live 여부 판단에 사용
- 파일이 지워지면 version이 증가함

**대신:**
- Free blocks에 대한 bitmap은 없음

### Segment Cleaning 동작

Segment cleaning은 파일 시스템과 **독립적**으로 동작합니다.

```
VFS ⟷ Flash file system ⟷ Flash memory
           ↓
    Segment cleaning
```

### Segment Cleaning Techniques

논문에서는 **write cost**라는 새로운 metric을 제안했습니다.

**When cleaner runs?**
- Free segments의 개수가 threshold 이하로 떨어지면

**How many segments to be cleaned?**
- A few of tens (몇십 개)

**Which segments to be cleaned?**
- **Write cost** 평가를 기반으로 선택

### Write Cost Metric

**정의:** 
새로운 데이터 1바이트를 쓸 때 디스크가 busy한 시간의 양

**수식:**
```
Write cost = (총 읽고 쓴 바이트) / (새로운 데이터의 바이트)
           = (read segs + write live + write new) / (write new)
           = [N + N*u + N*(1-u)] / [N*(1-u)]
           = (총 쓰기 량) / (유효 데이터 쓰기량)
```

여기서:
- N: segment 개수
- u: utilization (live data의 비율)

**Write cost의 의미:**
- Write cost = 1.0은 이상적: 디스크 bandwidth를 full로 사용
- Write cost = 2: 디스크 bandwidth의 50%만 사용
- Write cost = 10: 디스크 bandwidth의 1/10만 writing에 사용하고, 나머지는 seek, cleaning, rotational delay

**성능 분석:**
Live 한 녀석들의 개수(u)에 따라서 write cost가 늘어납니다. Utilization이 높을수록(live data가 많을수록) cleaning 오버헤드가 커집니다.

## Performance Estimation

그래프 분석:
- X축: Fraction alive in segment cleaned (μ) - 0.0 ~ 1.0
- Y축: Write cost

**결과:**
- **Log-structured:** Write cost가 거의 일정하게 낮음 (~2-4)
- **FFS today:** 약 10 정도
- **FFS improved:** 약 4 정도

LFS는 utilization에 따라 약간 증가하지만, 전통적인 파일 시스템에 비해 훨씬 효율적입니다.

## 요약

### LFS - 연구가 실용화된 좋은 보기

**배울 점:**

1. **출발점: Good observation**
   - 기존 파일 시스템의 문제를 정확히 파악
   - Small write와 seek overhead 문제 인식

2. **간단한 solution**
   - Append-only writing
   - Sequential logging
   - 복잡하지 않지만 효과적

3. **새로운 metric을 통한 분석: write cost**
   - 성능을 정량적으로 측정할 수 있는 지표 제시
   - 이를 통해 설계 결정의 타당성 입증

4. **실제 구현 및 실험**
   - 이론에 그치지 않고 실제 시스템으로 구현
   - Sprite OS에서 실제로 동작하는 것을 증명

### LFS의 현대적 의의

**플래시 메모리와의 관계:**
- LFS는 원래 디스크를 위해 설계되었지만
- 플래시 메모리의 특성(sequential write가 유리)과 완벽하게 맞아떨어짐
- 현대 플래시 파일 시스템(F2FS 등)의 기반이 됨

**핵심 개념:**
- **Append-only logging:** 새로운 데이터는 항상 끝에 추가
- **Garbage collection:** Stale 데이터를 주기적으로 정리
- **Wear-leveling:** 플래시의 수명 연장 (원래는 없었지만 플래시에서 추가됨)

## 추가 개념: Object Storage 동작

Object Storage의 내부 동작 방식을 이해하면 LFS와의 유사점을 발견할 수 있습니다.

### 메타데이터
- (버킷, 키, 위치 정보)
- 위치 정보: 복제, 버전, 조각 등

### 찾는 절차 (GET)
1. 클라이언트가 게이트웨이/프론트엔드에 `GET /bucket/key` 요청
2. 메타데이터에서 키를 조회 → 소유 노드 N개를 얻음
3. 해당 노드로 바로 읽기
4. 노드 내에서는 key→파일로 매핑
   - 대규모 시스템에서는 key→raw disk로 매핑

### 쓰기 (PUT)
1. `PUT /{bucket}/{key}` + 바디(객체 데이터)
2. 권한 검사 (버킷 정책 등)
3. 파티션/노드 배치 결정 (다중 복제 또는 조각화)
4. 다중 동시 기록 → 메타데이터 커밋(강한 일관성)
5. 응답으로 버전ID (버전닝 ON) 등 반환

## 정리: Flash와 LFS의 관계

| 특성 | Flash Memory | LFS |
|------|--------------|-----|
| **Write 방식** | Sequential write 선호 | Sequential logging |
| **Overwrite** | 불가능 (Erase 필요) | Append-only |
| **공간 관리** | Garbage Collection | Segment Cleaning |
| **Mapping** | FTL (LSN→PSN) | Inode Map |
| **Wear 관리** | Wear-leveling | 원래 없음 (플래시에서 추가) |

**결론:**
LFS는 1992년에 디스크를 위해 설계되었지만, 그 아이디어가 20년 후 플래시 메모리 시대에 다시 주목받게 되었습니다. 이는 좋은 아이디어가 시대를 초월한다는 것을 보여주는 훌륭한 사례입니다.

**핵심 통찰:**
- Sequential write가 random write보다 효율적이다
- Append-only 구조가 crash recovery를 단순화한다
- Garbage collection은 피할 수 없지만, 잘 설계하면 오버헤드를 최소화할 수 있다
- 새로운 metric(write cost)을 통해 성능을 정량적으로 분석할 수 있다

이러한 원칙들은 플래시 메모리, SSD, 그리고 현대의 대규모 분산 저장 시스템에서도 여전히 유효하며, F2FS, RocksDB, Cassandra 등 많은 현대 시스템의 설계 기반이 되고 있습니다.